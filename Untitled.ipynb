{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import praw\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(file_name,obj):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return file_name + ' is saved'\n",
    "\n",
    "def open_pickle(file_name):\n",
    "    with open(file_name, 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "    return obj\n",
    "\n",
    "def get_credentials(path):\n",
    "    \n",
    "    with open(path, 'rb') as handle:\n",
    "         cred = pickle.load(handle)\n",
    "    \n",
    "    return cred\n",
    "    \n",
    "    \n",
    "def retrieve_comment_and_post_count(reddit_data_dict,subreddit):\n",
    "    lookback_hours = 24\n",
    "\n",
    "    headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'\n",
    "        }\n",
    "    stats = {'comment':'num_of_comments', 'submission':'num_of_posts'}\n",
    "    url = \"https://api.pushshift.io/reddit/{}/search?&limit=1000&sort=desc&subreddit={}&before=\"\n",
    "\n",
    "    startTime = datetime.datetime.utcnow()\n",
    "    endTime = startTime - timedelta(hours=lookback_hours)\n",
    "    endEpoch = int(endTime.timestamp())\n",
    "\n",
    "    for stat in stats:\n",
    "        count = 0\n",
    "        breakOut = False\n",
    "        previousEpoch = int(startTime.timestamp())\n",
    "        while True:\n",
    "            newUrl = url.format(stat, subreddit)+str(previousEpoch)\n",
    "            json = requests.get(newUrl, headers=headers)\n",
    "            objects = json.json()['data']\n",
    "            for obj in objects:\n",
    "                previousEpoch = obj['created_utc'] - 1\n",
    "                if previousEpoch < endEpoch:\n",
    "                    breakOut = True\n",
    "                    break\n",
    "                count += 1\n",
    "\n",
    "            if breakOut:\n",
    "                reddit_data_dict[stats[stat]].append(count)\n",
    "                break\n",
    "\n",
    "def create_new_reddit_instance(credentials):\n",
    "    \n",
    "    reddit = praw.Reddit(client_id=credentials['client_id'],\n",
    "                         client_secret=credentials['client_secret'],\n",
    "                         user_agent=credentials['user_agent'],\n",
    "                         username=credentials['username'],\n",
    "                         password=credentials['password'])\n",
    "    \n",
    "    return reddit\n",
    "\n",
    "def add_data_to_dict(reddit_data_dict,time_stamp,sub,file_name):\n",
    "    #rate limit is one request per second (60 requests per min)\n",
    "    start_time = time.time()\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    reddit_data_dict['date'].append(current_timestamp)\n",
    "    reddit_data_dict['title'].append(subreddit.title)\n",
    "    reddit_data_dict['id'].append(subreddit.id)\n",
    "    reddit_data_dict['subscribers'].append(subreddit.subscribers)\n",
    "    retrieve_comment_and_post_count(reddit_data_dict,subreddit)\n",
    "    \n",
    "    save_pickle(file_name, reddit_data_dict)\n",
    "    sleep_time = 5 - (time.time() - start_time)\n",
    "    if sleep_time < 0:\n",
    "        sleep_time = 0.5\n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "\n",
    "\n",
    "def parse_recent_activity(current_timestamp, credentials, subreddit_names):\n",
    "    now = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    file_name = 'subreddit_scrape_' + now + '_dict.pkl'\n",
    "    \n",
    "    reddit_data = {\n",
    "        'date':[],\n",
    "        'title':[],\n",
    "        'id':[],\n",
    "        'subscribers':[],\n",
    "        'num_of_comments':[],\n",
    "        'num_of_posts':[] \n",
    "    }\n",
    "    \n",
    "    \n",
    "    reddit = create_new_reddit_instance(credentials)\n",
    "    \n",
    "    skipped = []\n",
    "    \n",
    "    for sub in subreddit_names:\n",
    "        try:\n",
    "            print(sub)\n",
    "            add_data_to_dict(reddit_data,current_timestamp,sub,file_name)\n",
    "            \n",
    "            if reddit.auth.limits['remaining'] == 0:\n",
    "                reddit = create_new_reddit_instance(credentials)\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            skipped.append(sub)\n",
    "            save_pickle('skipped_'+file_name, skipped)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    return reddit_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression-premium",
   "language": "python",
   "name": "regression-premium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
